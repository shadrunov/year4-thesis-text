\section{Vulnerability detection}
\subsection{Previous studies}

The research on the quality and effectiveness of vulnerability detection tools has already been attempted. One of the most comprehensive works was published by Omar Javed and Salman Toor in 2021 \cite{arxiv:1}. The approach described in the paper was based on several open source scanners (namely, Clair, Anchore, and Microscanner). After inspecting 59 Docker images for Java-based applications, the authors calculated detection coverage and detection hit ratio metrics and and concluded that the most accurate tool, Anchore, missed around 34\% of vulnerabilities. The metric used in this study, detection coverage, is in fact a well-known metric in data analysis usually referred to as recall. However, a major limitation of their work is the small number of images used to evaluate scanning tool performance. Additionally, the limited range of scanners examined and the reliance on a single metric for evaluating quality are also limitations.

Another study conducted by Liu et al. revealed the concerning state of image security on Docker Hub \cite{c:16}. The researchers analysed more than 2 million public images examining them for sensitive and potentially insecure parameters as well as malicious software and vulnerabilities. To detect malware, they used VirusTotal, and vulnerabilities were discovered using Anchore scanner. As a result, the researchers were able to detect several malicious images and discovered that vulnerability patching is often delayed, taking an average of about half a year for software developers to fix breaches. Despite its insightful revelations, this work relies solely on a single tool for static analysis.

In 2021, another vulnerability analysis of Docker Hub images was presented by Wist and her colleagues \cite{c:17}. In this study, 2500 images were analyzed, which helped to reveal several interesting findings. The number of vulnerabilities in recent Docker Hub images has been on the rise compared to images published earlier. The authors investigated the susceptibility of four categories of Docker Hub images: verified, certified, official, and community. It was found that certified images were more vulnerable, while official images appeared to be less so. Finally, no clear correlation could be found between the number of vulnerabilities and other image features, such as the number of downloads or the last update date. In this paper, the only used tool was Anchore Engine.

Another paper by K. Brady et al. described the CI/CD pipeline which combined static and dynamic analysers \cite{c:2}. A set of 7 images was submitted to Clair and Anchore scanners and original dynamic scanner based on Docker-in-Docker approach. The results clearly show the importance of dynamic detection method, however, no direct comparison of Clair and Anchore was conducted. Similar research was conducted in 2021, however, a pair of SonarQube and VirusTotal was used \cite{c:3}.

Massive research was conducted in 2020 by security company Prevasio. As the report states, all 4 million of public images from Docker Hub were scanned with static analyser (Trivy) and dynamic analyser, and more than half of all images were discovered to have critical vulnerabilities \cite{report:dynamic}. Significant number of images were containing dynamically downloaded payload, cryptominers and other malicious software.

Upon the review of the literature on static analysis of containerized applications, we can conclude that most experiments either do not focus on comparing the performance of existing scanning tools or have a limited number of images or scanners examined. In the following sections, we describe our experiment, which addresses these limitations.

\clearpage
\subsection{Static analysis}

Vulnerability scanners are special tools that identify known software vulnerabilities inside containers. Two approaches to this issue are known. First, it is possible to analyse versions of all the software in container image and check its safety. This approach is called static analysis, as it does not require any container to run, which speads up the process. However, static analysis has significant limitations, because the vulnerability databases take time to include recently discovered vulnerabilities, and container must not download any other packages besides what is included in the image. Finally, malicious but not yet reported packages cannot be discovered by static analysers. Nevertheless, this technology is extensively used in CI/CD pipelines to minimize risks of attack and container compromisation.

The opposite way to detect vulnerabilities in containers is to observe its runtime behaviour. Tools implementing this idea are called dynamic analysers. They may gather data about running processes, network and disk usage and discover suspicious activity patterns such as requests to C2 servers or compiling executables. This approach can effectively detect malicious images in the cases when static scanners are powerless. However, they require extensive resources and time usage, as each scan takes tens of seconds to complete, and may be unpredictable in special cases, when malicious applications disguise themselves and imitate normal behaviour unless started in the production environment. Further discussion will be held about static analysers.

The process of image attestation with static analyser may be described as following. First, the scanner compiles the list of all installed packages and libraries inside the image along with their versions. Next, each package is checked upon publically available vulnreability databases and marked as vulnerable or safe to use. This explains why various mistakes could easily occur during the detection process, or why reports from different scanners are not identical. Each scanner gets vulnerability information from various sources including per-distribution security advisories (Ubuntu Oval database, Debian Security Tracker, RHEL Oval database) or general vulnerability databases such as NVD. The information in these sources are of various degrees of relevance. Special treatment must be provided to vulnerabilities that will not be fixed due to its negligibility. Finally, package names may vary between distributions. All these reasons lead to false negative results, when a vulnerable package is marked as save. On the contrary, some packages are organised in groups, and incorrect treatment of the whole group because of one package can probably cause false positive results. Overall, the scanning report shoud be carefully considered in individually \cite{book:rice}.

\clearpage
\subsection{Existing tools}

The range of static vulnerability scanning tools available on the market is wide. From this extensive selection, we have chosen several for further evaluation. For the experiment, it was crucial that the scanning tool could run locally via the command line, as this allows for automation using Python scripts and the ability to process thousands of images for testing. Additionally, we wanted to ensure that the tool was either free or offered a sufficient trial period to meet our needs. After careful consideration, we selected the following tools: Clair, Trivy, Docker Scout, Anchore Grype, Snyk, and one provided by a cloud provider known as Google Artifact Registry.

\subsubsection{Clair}

Clair is a static analysis tool developed by CoreOS in 2015 to uncover vulnerabilities in containers that had previously remained unnoticed \cite{s:clair2015}. After CoreOS was discontinued in 2020, Clair can be used either as a standalone solution or as part of Project Quay, a container image registry that offers additional features such as image scanning \cite{s:what-is-clair}.

Clair performs a static analysis of container images. It scans each layer of the container image and identifies the packages contained within it. These packages are then evaluated against public vulnerability databases to identify any known potential security breaches. 

To identify vulnerabilities, Clair uses the following security databases \cite{s:clair-intro2}:

\begin{itemize}
    \item Ubuntu Oval database
    \item Debian Security Tracker
    \item Red Hat Enterprise Linux (RHEL) Oval database
    \item SUSE Oval database
    \item Oracle Oval database
    \item Alpine SecDB database
    \item VMware Photon OS database
    \item Amazon Web Services (AWS) UpdateInfo
    \item Open Source Vulnerability (OSV) Database
\end{itemize}

However, in local installations, Clair only recieves information from Ubuntu, Debian, Red Hat Enterprise Linux (RHEL), Alpine and the Open Source Vulnerabilities (OSV) databases.

Structurally, the Clair backend, which is composed of several daemons, such as Indexer, Matcher, and Notifier, runs in the background \cite{d:clairdeployment}. Upon startup, it downloads and maintains a local list of vulnerabilities that is stored in a Postgres database. To interact with the Clair backend, a special client tool called clairctl is used. clairctl pulls an image archive, pushes image layers to the Clair API, receives the results and presents them in a selected format. The results can be displayed on the screen or saved as a JSON file in a machine-readable format for later processing.

The latter feature simplifies the integration of Clair into the CI/CD pipeline for building and shipping containers. However, due to its design, Clair requires a significant amount of time to fully compile its vulnerability database, ranging from 20 to 30 minutes. To speed up this process, enthusiasts attempted to distribute preconfigured database images. A corresponding project can be found on GitHub (\url{https://github.com/arminc/clair-local-scan}). However, the feasibility of this approach is questionable due to the lack of support.

To run Clair locally, we used the Docker images from the official repository (\url{https://github.com/quay/clair}). A special docker-compose file, listed in Appendix \ref{appendix:clair-compose}, helps to run the database and scanner containers. The \texttt{clairctl} command-line tool, which is included with the Clair package, pulls the image from Docker Hub and passes it to the Clair backend. The deployment process is described in the Clair documentation (\url{https://quay.github.io/clair/howto/getting_started.html}). An example of a command that needs to be executed to scan an image is provided below:

\begin{listing}[htp]
    \centering
    \begin{minipage}{0.95\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
clairctl report --out json fluent/fluent-bit:2.0.8-debug > report.json
        \end{minted}
    \end{minipage}
    \caption{Run Clair scanner}
    \label{lst:clair}
\end{listing}

Upon completing the analysis, Clair generates a report that includes several sections, including information about the examined packages, the detected Linux distribution in the base image, and a list of vulnerabilities. For each vulnerability, Clair provides information about its identification in a database (for example, the CVE number), severity and the packages where the vulnerability was found. Additionally, the report indicates whether a fix for the vulnerability has already been released.

Overall, Clair is one of most established solutions for static container analysis and can be effectively used to gain insight into what is built and deployed in production systems. While its modular architecture may make the installation process more complicated, it provides a wide range of customization options for different environments and scalability for large-scale deployment systems. Later, we will explore the results of using Clair and compare them to other scanning tools.

\subsubsection{Trivy}

Trivy is an open-source scanner developed by Aqua Security, a cloud security company founded in 2015. It is a replacement for Microscanner, another vulnerability detector developed previously by the same company. The main advantage of Trivy over other scanning tools is the simplicity of installation. To begin scanning, developers simply need to download an executable file from the vendor's website or the package manager. Trivy can scan not only container images, but also file systems, Git repositories, Virtual Machine images and Infrastructure as code files \cite{d:trivystart}. In addition to detecting vulnerabilities, Trivy also checks for misconfigurations, such as secrets or sensitive information stored in plain text.

To demonstrate the scanning process, the Trivy tool can be executed by using the command provided in Listing \ref{lst:trivy}.

\begin{listing}[htp]
    \centering
    \begin{minipage}{0.75\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
trivy image --format sarif -o report.json golang:1.12-alpine
        \end{minted}
    \end{minipage}
    \caption{Run Trivy scanner}
    \label{lst:trivy}
\end{listing}

Internally, Trivy collects information about vulnerabilities from a wide range of sources, including security databases from major Linux distributions. It supports approximately 15 base images and, in addition, language-specific components such as libraries for approximately 12 programming languages, including Python, Go and C++ \cite{d:trivyvulnerability}. It is especially impressive considering that all these processes take place automatically and do not require any user intervention.

Trivy can also be used as part of a CI/CD pipeline. It is compatible with major continuous integration systems, such as GitHub Actions, GitLab CI and Travis CI. To facilitate integration with other tools and environments, Trivy provides various output formats, including human-readable text, JSON, and SERIF, which is a standard format for static code analysis. The JSON report contains information about the tool that was used for scanning, the vulnerabilities found their location in the image and the severity level.

In conclusion, Trivy is a recently developed and actively maintained open-source tool that focuses on the user-friendly approach. It provides wide support for various distributions, programming languages and platforms. With its extensive range of scanning features, Trivy makes it an ideal choice for setting up static analysis on containerized applications.


\subsubsection{Docker Scout}

Docker Scout is a new container security solution from Docker. It replaces the legacy Docker Scan tool and was made available to the general public in October 2023 \cite{s:scoutga}. 

Docker Scout can be used as a standalone plugin. To run Docker Scout plugin locally, it must be installed and then invoked as shown in Listing \ref{lst:scout}. Alternatively, Docker Scout can be integrated with Docker Hub, where developers can view the results of scanning their images from the cloud. An example of the Docker Scout dashboard can be seen in Figure \ref{img:scout}. It is also worth noting that the cloud version of Docker Scout requires a subscription to Docker. Otherwise, users can only use three repositories free of charge.


\begin{listing}[htp]
    \centering
    \begin{minipage}{1\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
# installation
curl -fsSL \
    https://raw.githubusercontent.com/docker/scout-cli/main/install.sh \
    -o install-scout.sh
sh install-scout.sh

# usage
docker scout cves --format sarif --output report.json \
    stakater/reloader:SNAPSHOT-PR-586-UBI-0db6f802
        \end{minted}
    \end{minipage}
    \caption{Run Docker Scout scanner}
    \label{lst:scout}
\end{listing}

Otherwise, Docker Scout supports integrations with several third-party systems, such as container registries (JFrog Artifactory, Amazon and Azure container regstries) and CI/CD platforms (GitHub Actions, GitLab and Jenkins).

As a data source, Docker Scout aggregates vulnerability information from various sources, including Linux distribution advisories, language-specific databases, GitHub and GitLab advisory systems as well as the National Vulnerability Database \cite{d:scoutdb}. Unlike other scanner tools, Docker Scout utilizes Package URLs (PURLs) rather than the Common Product Enumeration (CPE) system, which, according to its developers, significantly reduces the likelihood of false positive results.

\image{scout.png}{Docker Scout dashboard}{img:scout}{1}
% \FloatBarrier

For the purposes of our evaluation, we primarily focus on the \texttt{docker scout cves} command, which generates a vulnerability report for a selected image. This command is not limited to images, but can also be used for OCI bundles and local files. The tool's output can be generated in plain text, SARIF or SBOM (Software Bill of Materials) formats. The SERIF-formatted report is compatible with other tools.


\subsubsection{Anchore Grype}

Anchore Grype is a vulnerability scanning tool for container images and file systems, maintained by Anchore since 2020 \cite{s:anchore}. As a static analysis tool, Grype examines the contents of container images to identify known vulnerabilities in all major operating system and language-specific packages \cite{gh:grype}.

To simplify the user experience, Grype is distributed as a single binary from the release pages of package managers. Listing \ref{lst:grype} demonstrates how Grype can be installed and used in a local environment. Additionally, Grype can also be used as part of a GitHub Actions pipeline.


\begin{listing}[htp]
    \centering
    \begin{minipage}{1\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
# installation
curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh \
    | sh -s -- -b /usr/local/bin

# usage:
grype stakater/reloader:latest -o json > report.json
        \end{minted}
    \end{minipage}
    \caption{Run Docker Scout scanner}
    \label{lst:grype}
\end{listing}

This command generates a compliance report that provides information about vulnerabilities found in the Grype database, as well as information about the image metadata and Linux distribution used to create the image. Each vulnerability is described with a CVE identifier, severity level, and CVSS score. The report also includes information on how to fix the vulnerability and links to additional resources. In some cases, related vulnerabilities may be included, such as when an advisory from GitHub has a corresponding CVE entry in the national vulnerability database. Another useful section of the report provides details about each match, explaining the exact information about the package that led to the detection of the vulnerability.

Grype supports a variety of formats, including JSON, SARIF, and tabular or XML, as well as other formats. Grype collects data from various publicly available vulnerability sources, such as advisories for popular Linux distributions and the National Vulnerability Database (NVD). The local database is automatically maintained by the scanner. However, developers can also use special commands to update or manually check the database.

Anchore Grype is a great tool for both local development and in CI/CD pipelines. It is free to use and produces detailed and accurate results. we will evaluate the quality of Grype reports for Docker images that are used in our experiments.


\subsubsection{Snyk}

Snyk is a comprehensive solution for securing open-source libraries, application code, container images and Kubernetes applications. It also provides infrastructure as code file protection, such as Terraform configurations. As a security company, Snyk was founded in 2015, during the rapid development of containerized technologies. Developers can access Snyk through a web UI or by using the snyk CLI tool \cite{gh:snyk}. A subscription is also available, which provides additional features such as private repository evaluation.


As other major scanning tools, Snyk can be integrated into the CI/CD pipelines of Jenkins and GitHub Actions, as well as popular IDEs such as VS Code and JetBrains. Additionally, Snyk supports Git repositories, cloud providers, and package managers, including npm and the Nexus repository. However, for the objectives of our evaluation, it is sufficient to run Snyk locally using the command line, as demonstrated in Listing \ref{lst:snyk}. Unlike other scanning tools, Snyk requires authentication with a developer account using an API token.

\begin{listing}[htp]
    \centering
    \begin{minipage}{0.9\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
# installation
curl --compressed https://static.snyk.io/cli/latest/snyk-linux -o snyk
chmod +x ./snyk
mv ./snyk /usr/local/bin/

# authentication
snyk auth de452e65-...

# usage:
snyk container test --sarif-file-output=report.json ubuntu:16.04 -d
        \end{minted}
    \end{minipage}
    \caption{Snyk scanner}
    \label{lst:snyk}
\end{listing}

The results from Snyk can be presented in a serial format and easily processed by other tools. Snyk discovers vulnerabilities by relying on advisories from operating system maintainers and detects vulnerabilities in images based on all major Linux distributions and a range of programming languages.

In addition to its image scanning capabilities, Snyk provides a range of tools to help ensure the security of the software development process at every stage of the lifecycle. Overall, Snyk is a valuable asset for both individual developers and teams.

\subsubsection{Google Artifact Registry}

In the previous section, we discussed the tools that are used for scanning in a local environment or as a part of deployment pipeline. These tools are mostly open source (except for Docker Scout), and they operate with a local database of vulnerability detection rules. Next, we will discuss cloud-based scanning utilities. These are offered as SaaS platforms by cloud providers and have an operating cost, usually based on the number of scans. 

Most providers offer such solutions, including image scanning in Amazon Elastic Container Registry, Azure Container Registry and Yandex Container Registry. We will be focusing on a specific cloud-based scanner that is integrated with Google Artifact Registry. Artifact Registry is a unified storage solution for images and packages from different sources. When vulnerability scanning is enabled, this service automatically scans images as they are pushed to the registry to detect known security vulnerabilities and exposures.

Static analysis provided by Google is an integral part of its registry. Therefore, Artifact Analysis scans for vulnerabilities, dependencies, and licenses only the images pushed or pulled to the cloud. To ensure a constant level of security, two approaches are used: on-push scanning and continuous analysis \cite{d:gcp}. Firstly, each new image is analyzed upon uploading to Artifact Registry. After the scan is completed, the registry generates a report containing detected CVEs, their severity and CVSS scores, whether a fix is available and affected packages. The report also provides information about licenses and dependencies. The report can be accessed through the Google Cloud console, and an example is provided in Figure \ref{img:gar}.


\image{gar.png}{Google Artifact Registry}{img:gar}{1}
% \FloatBarrier

After the initial analysis, the registry will continue to update the image evaluation as new information becomes available from vulnerability sources every day. Google gathers information from Linux distributions advisories and the GitHub Advisory Database for language packages.

For convenience, the report can be retrieved using the CLI tool called \texttt{gcloud}. To obtain scan results for a set of Docker Hub images, we have implemented the following process. Firstly, we set up Artifact Registry as a proxy for Docker Hub, which means it downloads and caches images in the registry upon user request to Docker Hub. When an image is processed through the registry, it is scanned and a report is generated and saved in the cloud. Meanwhile, to release disk space, the image is deleted from the local machine as it is no longer needed. After a while, the second iteration is performed to query the results of the image evaluation using the \texttt{gcloud} tool. The necessary commands are given in Listing \ref{lst:gcloud}.

\begin{listing}[htp]
    \centering
    \begin{minipage}{0.85\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
# pull image
docker pull west1-docker.pkg.dev/proud/shadrunov/ubuntu:16.04

# retrieve security report
gcloud artifacts docker images describe \
    west1-docker.pkg.dev/proud/shadrunov/ubuntu:16.04 \
    --show-package-vulnerability \
    --format json > report.json

# remove image from local machine
docker image rm west1-docker.pkg.dev/proud/shadrunov/ubuntu:16.04
        \end{minted}
    \end{minipage}
    \caption{Vulnerability scanning with \texttt{gcloud} tool}
    \label{lst:gcloud}
\end{listing}

Overall, the Google Cloud Platform offers an easy-to-use tool for those who prefer sticking to cloud solutions. As part of its design, it provides continuous monitoring for the security of images stored in the cloud, which is an advantage over one-time scanning approach. Other tools are also striving to implement this feature, as was discussed by the example of Docker Scout, which has a similar feature when applied to cloud repositories.

After discussing the various scanning solutions available on the market and their capabilities and limitations, we can now proceed to describe how we collect data from each scanner report over a set of several thousand Docker Hub images.


\clearpage

\subsection{Methodology}

The process of data collection and analysis for this research project can be divided into several stages. First, a set of container images and tags were created for scanning. For this purpose, the Docker Hub API was used. Then, each image was submitted to a scanning tool, and the vulnerability report was saved in JSON format on the disk. After scanning, the reports were combined, and the number of vulnerabilities in each image was calculated. Based on these calculations, metrics for the detection quality of each scanner were calculated.

\subsubsection{Images selection}

To select the most popular images from Docker Hub according to the number of pulls, the following algorithm was developed.

First, First, we compiled a list of search queries consisting of all possible combinations of two Latin letters starting from \texttt{aa} and finishing with \texttt{zz}. Each of the $26 \cdot 26 = 676$ combinations was then used to query the list of corresponding images from Docker Hub, as shown in Listing \ref{lst:images}. The result was then saved to the JSON file.
\begin{listing}[htp]
    \centering
    \begin{minipage}{0.8\linewidth}
        \begin{minted}[linenos=true, tabsize=4]{python}
def get_page(page, query):
url = "https://hub.docker.com/api/content/v1/products/search"
params = {
    "page_size": 100,
    "q": query,
    "source": "community",
    "type": "image",
    "page": page
}
response = requests.get(url, params=params)
response.raise_for_status()
data = response.json()
return data["summaries"]
        \end{minted}
    \end{minipage}
    \caption{Query images}
    \label{lst:images}
\end{listing}

The next step is to combine the search results from each query and sample a reasonably sized subset of the most popular images. We parse each JSON file and remove duplicate images. After all, the set of 3694651 images is sorted by \texttt{popularity} parameter and approximately 1200 images are selected from the top of the list and saved for further processing.

\subsubsection{Tags selection}

Each image exists in multiple versions which are referred to as image tags. We have to select a number of tags for each image to scan. Tags could also be built for a specific architecture and OS platform. We are primarily interested in \texttt{amd64} and \texttt{Linux} tags. 

The request used for generating the list of tags for each image is demonstrated in Listing \ref{lst:tags}. We randomly select 3 tags for each image that satisfies the mentioned requirements. As the result, the set of approximately 3200 tags is composed and stored in file.

\begin{listing}[htp]
    \centering
    \begin{minipage}{1\linewidth}
        \begin{minted}[linenos=true, tabsize=4]{python}
pref, suf = image.split("/")
link = f"https://hub.docker.com/v2/namespaces/{pref}/repositories/{suf}/tags"
res = requests.get(link)
res = res.json()
up = res["count"]
iteration = 0
total = 0
while total < 3 and iteration < 100:
    page_num = random.randint(1, up)
    response = requests.get(link, params={"page": page_num, "page_size": 1})
    tag = response.json()["results"][0]
    if "amd64" in [i["architecture"] for i in tag["images"]]:
        tags.append(tag)
        total += 1
    iteration += 1
        \end{minted}
    \end{minipage}
    \caption{Query tags}
    \label{lst:tags}
\end{listing}

\subsubsection{Scan images}

This step was performed on a virtual machine with 16 virtual CPU cores and 32 GB of RAM. As a general rule, each scanning tool must be installed on the machine before each image is passed to the tool using a command line interface. The installation process was already mentioned in the previous section.

\subsubsection{Analysis of obtained scan reports}

After submitting the set of images to each of the scanning tool, the reports must undergo the further analysis to determine the effectiveness of the studied software. As the results are presented in JSON format, python libraries such as \texttt{json} and \texttt{pandas} provide us with enough features to calculate the desired quality metrics. 

\subsubsection*{Data preparation}

As each scanner produces results in a different format, the data needs to be prepared and combined for further analysis. First, we identified a list of tags that were reported by all the scanning tools. Then, 16 of these tags were excluded, leaving a total of 3191 scanned tags.

Then, each result from the scanner was reduced to the necessary information, creating a scan record. Each record is marked with \texttt{"uid"} field. The main issue is the need to identify vulnerabilities reported in the current entry. Therefore, we scan the report and try to find the mentioned CVE ID or ID of a GitHub Security Advisory or Red Hat Security Advisory. Next, the severity category is extracted using the CVSS v3 scoring scale (None, Low, Medium, High, Critical) or another similar categorical system. Additionally, the package that contains the reported vulnerability is saved, along with the base layer distribution of the image. Finally, if such information is available, we obtain data about the available fixes. The exact steps for correctly processing the data from each scanner depend on the scanning report format. The example of scan record is provided in Listing \ref{lst:scanrecord}.


\begin{listing}[htp]
    \centering
    \begin{minipage}{0.7\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{json}
{
    "id": "CVE-2023-20883",
    "cve_id": "CVE-2023-20883",
    "sas": [
        "GHSA-xf96-w227-r7c4",
        "CVE-2023-20883"
    ],
    "severity": "high",
    "package": "spring-boot-autoconfigure",
    "distro": [
        "alpine",
        "3.17.3"
    ],
    "fixed": true,
    "uid": "grype77"
}
        \end{minted}
    \end{minipage}
    \caption{Format of scan record}
    \label{lst:scanrecord}
\end{listing}

\subsubsection*{Data merging}

The next task is to match the records from each tool. The obtained data demonstrated, that in case of certain scanners (namely, Clair) several records may point to the same vulnerability. The difference is only the reported packages, which in fact could be the chain of dependencies and be in fact the same place of the system. Such occurrences should be merged together. Then, using field \texttt{"sas"} which contains all the identificators of vulnerability found in a specific record, record from different scanners should be compared and linked in case they are equivalent. After all, we combine the set of aggregated records, where each record holds pointers to the corresponding scan records of every scanning tool. The example is given in Listing \ref{lst:aggrrecord}.


\begin{listing}[htp]
    \centering
    \begin{minipage}{0.7\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{json}
{
    "uid": "aggr10",
    "links": {
        "clair": ["clair10"],
        "gc": ["gc16"],
        "grype": ["grype67", "grype91"],
        "scout": ["scout16", "scout54", "scout55"],
        "snyk": ["snyk13", "snyk31", "snyk38"],
        "trivy": ["trivy44", "trivy57"]
    },
    "conf": {
        "clair": true,
        "gc": true,
        "grype": true,
        "scout": true,
        "snyk": true,
        "trivy": true
    },
    "conf_count": 6,
    "conf_count_others": 5,
    "conf_rate": 1.0
}
        \end{minted}
    \end{minipage}
    \caption{Format of aggregated record}
    \label{lst:aggrrecord}
\end{listing}

After forming aggregated records, we need to determine if the vulnerability described in a record was detected by more than one scanner. We assume that the more tools detect a particular vulnerability, the more likely it is that it actually exists, and we can consider this report as the truth. For example, the record in Listing \ref{lst:aggrrecord} contains the vulnerability that has been reported by every scanning tool. We mark the existence of the report as \texttt{True} in the \texttt{"conf"} section. Next, we calculate the number of reports, which in this case equals to 6. Since at least one scanner must have reported a vulnerability in order to create an aggregated record, we can obtain the number of other scanners ``confirming'' the reliability of the report by subtracting 1 from the total number of confirmations. Lastly, the confirmation rate is calculated by dividing the number of other scanners that confirm the record by the total number of other scanners. In this example, 5 out of the 5 other scanners have validated the record, giving a confirmation rate of 1.0.


\subsubsection*{Data validation}

Firstly, each reported record from the scanner can be assigned to a category based on its validation by other scanners. The record falls into one of the following four categories:

\begin{itemize}
    \item \textbf{True Positive (TP)} — a record is TP if reported by a scanner and confirmed by other scanners. This is the correct result (actually present vulnerability) that the scanner has successfully identified.
    \item \textbf{False Positive (FP)} — a record is FP if reported by a scanner and not confirmed by other scanners. This is an incorrect result that was mistakenly produced by the scanner.
    \item \textbf{True Negative (TN)} — a record is TN if not reported by a scanner and confirmed by other scanners. This is an existing vulnerability that should have been identified and reported by the scanner, but was missed.
    \item \textbf{False Negative (FN)} — a record is FN if not reported by a scanner and not confirmed by other scanners. This is a correct result (non-existing vulnerability), which the scanner did not report and should not have reported.
\end{itemize}

In addition, we must also introduce the measure of sensitivity - the minimum number of other tools required to confirm the existence of a record. In the following analysis, we will use the minimum number of other scanners equal to 1 and 2.

So, now we can create a list of true positives, false positives, true negatives, and false negatives for each scanner. To do this, we compare the number of confirmations with the minimum requirement and see if the scanner detected this vulnerability. For example, if the number of confirmations is 4 out of 5, and the current scanner confirmed it as well, then it is a true positive. If the current scanner missed the vulnerability, it would be a false negative. If the vulnerability was confirmed by only one scanner, and that scanner is actually the one being evaluated, then it would be false positive. Otherwise, it would be true negative. This way, each scanner report is validated by other scanners.

\subsubsection*{Quality metrics}

Based on the classification of reports into the four categories described, it is possible to calculate metrics of accuracy for each scanner. In our work, we will use three well-known metrics from data analysis. First, we rely on precision and recall. Precision and recall are two of the most commonly used metrics to evaluate the performance of recognition tasks. They provide scores that are expressed as fractions \cite{a:recall}.

Precision is the number of correct results (true positives) relative to the number of all results. It described how many retrieved elements are relevant, or, in our case, how many reported vulnerabilities are actually present.

\begin{align*}
    Precision \ = \ \frac{Relevant \ retrieved \ elements}{All \ retrieved \ elements} \ = \ \frac{TP}{TP + FP}
\end{align*}


Recall is the number of correct results relative to the number of expected results. It described how many relevant elements were retrieved, or how many of existing vulnerabilities were reported by the scanner.

\begin{align*}
    Recall \ = \ \frac{Relevant \ retrieved \ elements}{All \ relevant \ elements} \ = \ \frac{TP}{TP + FN}
\end{align*}

It is also known that these metrics are not independent. For example, as a general rule, the higher the precision of a tool, the lower the recall it exhibits. In other words, if a tool guesses all the relevant elements, it will miss some other relevant ones. Conversely, if a tool detects all existing elements, it may also detect some non-relevant ones. We will observe this pattern in our findings.

As we understand the specifics of both precision and recall and how they characterise the evaluated tools, it is hard to compare tools based on two values. The performance can be unified into a single value called $F_1$ score. $F_1$ score is defined as the harmonic mean of precision and recall:

\begin{align*}
    F_1 \ = \ 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
\end{align*}

After calculating these metrics for each image, we can obtain the average results for each tool. The results of the evaluation will be discussed in the next section.

\FloatBarrier
\clearpage

\subsection{Findings}

In this section, we will discuss the results of the analysis of the data obtained from the scanners. The Python notebooks created for this purpose can be found on GitHub: \url{https://github.com/shadrunov/year-4-thesis}.

\subsubsection{Exploring the data}

As we previously mentioned, we have collected 1183 of the most popular images, based on the number of times they were pulled. The top 15 images are displayed in Table \ref{tab:topimages}, and the distribution of images according to the number of pulls is shown in Figure \ref{img:imagespull}. 

The data showed that most of the evaluated images were built on top of Alpine Linux, a lightweight distribution well-suited for containers (438 images). In addition, 249 and 206 images were based on Debian and Ubuntu systems, respectively. This distribution is illustrated in Figure \ref{img:imagesdistro}.


\begin{table}[hbt]
    \centering
    \begin{tabular}{| p{0.02\linewidth} | p{0.3\linewidth} | p{0.5\linewidth} | p{0.08\linewidth} |}
        \hline
        \centering \textbf{}     & \centering \textbf{Image}              & \centering \textbf{Description}                                    & \centering\arraybackslash \textbf{Pulls} \\ \hline
        1  & \texttt{stakater/reloader}     &                                                                                                 & 10B+  \\ \hline
        2  & \texttt{fluent/fluent-bit}     & Fluent Bit, lightweight logs and metrics collector and forwarder                                & 10B+  \\ \hline
        3  & \texttt{istio/pilot}           & Istiod (formerly known as Pilot)                                                                & 10B+  \\ \hline
        4  & \texttt{istio/proxyv2}         & Istio proxy                                                                                     & 10B+  \\ \hline
        5  & \texttt{datadog/agent}         & Docker container for the new Datadog Agent                                                      & 10B+  \\ \hline
        6  & \texttt{containrrr/watchtower} & A process for automating Docker container base image updates.                                   & 1B+   \\ \hline
        7  & \texttt{curlimages/curl}       & official docker image for curl - command line tool and library  for transferring data with URLs & 1B+   \\ \hline
        8  & \texttt{istio/operator}        & Istio in-cluster operator                                                                       & 1B+   \\ \hline
        9  & \texttt{envoyproxy/envoy}      & Cloud-native high-performance edge/middle/service proxy                                         & 1B+   \\ \hline
        10 & \texttt{jenkins/jenkins}       & The leading open source automation server                                                       & 1B+   \\ \hline
        11 & \texttt{grafana/grafana}       & The official Grafana docker container                                                           & 1B+   \\ \hline
        12 & \texttt{bitnami/postgresql}    & Bitnami PostgreSQL Docker Image                                                                 & 1B+   \\ \hline
        13 & \texttt{timberio/vector}       & A High-Performance Logs, Metrics, and Events Router                                             & 1B+   \\ \hline
        14 & \texttt{bitnami/kubectl}       & Bitnami Docker Image for Kubectl                                                                & 1B+   \\ \hline
        15 & \texttt{prom/node-exporter}    & prom/node-exporter                                                                              & 1B+   \\ \hline
    \end{tabular}
    \caption{Most pulled images}
    \label{tab:topimages}
\end{table}


For the set of approximately one thousand images, we retrieved 3207 tags from Docker Hub. After submitting each tag for scanning, we successfully received results for 3191 of these tags. Additionally, for each tag, we obtained the date of the latest update. As the data demonstrates, the majority of the images retrieved were updated recently. Indeed, the highest number of tags were updated in 2023 (741 tags) and the lowest in 2016 (only 72 tags), as can be seen in Figure \ref{img:tagsyear}. This signifies that the majority of images from our dataset received updates in recent years.

\image{imagespull.png}{Images distribution by the number of pulls}{img:imagespull}{0.5}

\image{imagesdistro.png}{Images per Linux distribution}{img:imagesdistro}{0.53}

\image{tagsyear.png}{Number of images per the year of last update}{img:tagsyear}{0.53}

\FloatBarrier
\clearpage

\subsubsection*{Examined Vulnerabilities}

Now, we should move on to the aggregated records we have obtained to analyze the vulnerabilities contained in each image.

After preparing the data, approximately 700,000 entries with vulnerabilities were obtained. Each entry describes a specific vulnerability in a package in a particular image. In total, nearly 15,000 unique vulnerabilities were identified. The most common vulnerabilities among the evaluated images are listed in Table \ref{tab:freqcve}. It can be observed that six of the most prevalent vulnerabilities are related to security breaches in the OpenSSL package, which was discovered in 2023. This indicates that the OpenSSL package is included in the base package of operating systems and its vulnerable version is present in up to half of all evaluated images.


\begin{table}[hbt]
    \centering
    \begin{tabular}{| p{0.2\linewidth} | p{0.4\linewidth} | p{0.1\linewidth} | p{0.1\linewidth} | p{0.08\linewidth} |}
        \hline
        \centering \textbf{CVE} & \centering \textbf{Description} & \centering \textbf{CVSS} & \centering \textbf{Package} & \centering\arraybackslash \textbf{Tags} \\ \hline 
        CVE-2024-0727 & OpenSSL vulnerability; a remote intruder can crash an application with a specially crafted PKCS12 file & Medium & openssl & 1487 \\ \hline
        CVE-2023-44487 & HTTP/2 protocol that allows attackers to crash servers by rapidly resetting many connections & High & nghttp & 1440 \\ \hline
        CVE-2023-5678 & OpenSSL vulnerability leading to DoS attacks by providing specially crafted X9.42 DH-keys & Medium & openssl & 1398 \\ \hline
        CVE-2023-3446 & OpenSSL vulnerability leading to DoS attacks by providing excessively long DH-keys & Medium & openssl & 1341 \\ \hline
        CVE-2023-3817 & OpenSSL vulnerability leading to DoS attacks by making the process of checking DH keys very slow & Medium & openssl & 1260 \\ \hline
        CVE-2024-2961 & Overflow vulnerability in older versions of glibc when working with Chinese encodings & None & glibc & 1238 \\ \hline
        CVE-2023-2650 & OpenSSL vulnerability leading to DoS attacks by slowing down cryptographic operations & Medium & openssl & 1231 \\ \hline
        CVE-2016-2781 & chroot vulnerability allowing local users to escape to the parent session & Medium & coreutils & 1227 \\ \hline
        CVE-2023-50495 & Segfault vulnerability in ncurses library & Medium & ncurses & 1221 \\ \hline
        CVE-2023-0465 & OpenSSL vulnerability allowing malicious certificate authorities to bypass certificate policy checks & Medium & openssl & 1192 \\ \hline
    \end{tabular}
    \caption{Top detected vulnerabilities}
    \label{tab:freqcve}
\end{table}

Then, we analyzed how the vulnerabilities were distributed in relation to the age of the tags. We found that the maximum number of vulnerabilities corresponded to tags that were about 4 years old and were updated in 2020. Overall, the histogram in Figure \ref{img:vulnstagyear} resembled a normal distribution.

\image{vulnstagyear.png}{Number of vulnerabilities per the year of last update}{img:vulnstagyear}{0.7}

In terms of the age of the vulnerabilities themselves, Figure \ref{img:vulnscveyear} illustrates the age distribution of vulnerabilities in the examined images. It is evident that the vast majority of detected vulnerabilities were reported between 2014 and 2024. However, some breaches date back to 2001. Additionally, it is notable that the number of vulnerabilities tends to increase each year. Examining the oldest detected vulnerabilities, we have compiled Table \ref{tab:oldcve}. This table indicates that the oldest vulnerabilities were initially discovered more than twenty years ago. These vulnerabilities affect well-known and widely used packages such as the Apache HTTP server and the krb5 library, which are required for interacting with domain infrastructure. Nevertheless, the number of images affected by these vulnerabilities is relatively low, due to the fact that the vulnerable versions of these packages are outdated to be included in modern images.

\image{vulnscveyear.png}{Number of vulnerabilities and the CVE date}{img:vulnscveyear}{0.7}


\begin{table}[hbt]
    \centering
    \begin{tabular}{| p{0.2\linewidth} | p{0.5\linewidth} | p{0.12\linewidth} | p{0.08\linewidth} |}
        \hline
        \centering \textbf{CVE} & \centering \textbf{Description} & \centering \textbf{Package} & \centering\arraybackslash \textbf{Tags} \\ \hline 
        CVE-2001-1534 & Apache mod\_usertrack vulnerability to bypass authentication by forgering session ID & apache2 & 13 \\ \hline
        CVE-2002-1976 & Linux kernel ifconfig vulnerability allowing intruders to place a network interface in promiscuous mode for eavesdropping packets & net-tools & 65 \\ \hline
        CVE-2002-2439 & gcc vulnerability allowing attackers to crash the system or execute arbitrary code due to an integer overflow & libgcc & 3 \\ \hline
        CVE-2002-2443 & krb5 vulnerability allowing to cause DoS attacks by sending a specially crafted UDP packet that triggers an infinite loop & krb5 & 3 \\ \hline
        CVE-2003-1307 & Apache mod\_php vulnerability allowing local users to control the server by sending signals and using file descriptors & apache2 & 15 \\ \hline
        CVE-2003-1580 & php phf library vulnerability allowing remote code execution on the server by sending a crafted HTTP request that triggers a stack-based buffer overflow & apache2 & 15 \\ \hline
        CVE-2003-1581 & Apache httpd vulnerability allowing malicious code injection into server log files by forgering DNS responses & apache2 & 15 \\ \hline
        CVE-2004-0423 & FTP server (wuftpd) vulnerability allowing RCE on the system through buffer overflows & ssmtp & 2 \\ \hline
        CVE-2004-0971 & krb5 vulnerability allowing local users to overwrite files on the system due to temporary files mishandling & krb5 & 140 \\ \hline
        CVE-2005-4890 & Shadow and sudo vulnerability allowing attackers to hijack a user terminal session & shadow & 3 \\ \hline
    \end{tabular}
    \caption{Oldest vulnerabilities}
    \label{tab:oldcve}
\end{table}

To further explore the correlation between the age of vulnerabilities and the age of tags, we should look at Figure \ref{img:vulnscveyearsc}. In this chart, each detected vulnerability is plotted on the chart according to the year of the last tag update and the year the vulnerability was discovered. It can be seen that the dots are slightly aligned with a diagonal line, which means that most images contain vulnerabilities that were discovered at the time of tag creation or afterwards. This is understandable, as when a tag is updated, it usually includes new versions of packages with security patches, so it is unlikely that an image would contain vulnerabilities discovered prior to its creation. However, after an image has been built and stored in a registry, its packets may still be found to be vulnerable in the same or subsequent years.

Furthermore, the chart demonstrates that after 2020, the concentration of vulnerable packages increased. This can be seen in previous charts, and it can be attributed to the overall development of security practices and vulnerability searching and reporting, in particular.


\image{vulnscveyearsc.png}{Correlation between age of vulnerabilities and images}{img:vulnscveyearsc}{1}

Moving on to package analysis, Table \ref{tab:packages} depicts the distribution of popular packages. To begin with, (a) subtable lists the packages sorted by their occurrences in images and occurrences of vulnerabilities in them. In other words, this is a measure of both package popularity and susceptibility to breaches. As can be seen, some of the top packages in this benchmark are Linux libraries, curl and openssl. Subtable (b) provides information about the number of CVEs detected in each package. It is clear from this data that most reported vulnerabilities were found in the Linux kernel and Linux libraries. This highlights the importance of regularly updating the base layer of images used in production environments. Additionally, tools such as MySQL and OpenJDK, as well as browser engines like Firefox and Chromium, also tend to have a high number of detected breaches. Lastly, (c) subtable shows the popularity of packages among the evaluated images.Thus, openssl was found in 2366 of the 3200 tags evaluated. As it is part of most Linux distributions, it is a highly widespread package. Next, the ncurses, zlib and glibc libraries, as well as the curl and systemd packages, were found in approximately half of the evaluated tags. The scripting language Perl and the bash command interpreter were found in about one third of the images evaluated.

\begin{table}[htb]
    \centering
    {\small
    \subfloat[Popularity of packages]{
    \begin{tabular}{lr} \toprule
        \textbf{Package} & \textbf{Entries} \\ \midrule
        linux & 53139 \\
        binutils & 38896 \\
        curl & 32641 \\
        openssl & 31684 \\
        stdlib & 29512 \\
        glibc & 26843 \\
        vim & 25297 \\
        kernel & 23179 \\
        imagemagick & 17779 \\
        tiff & 16891 \\
        systemd & 13052 \\
        expat & 12627 \\
        sqlite & 12201 \\
        ncurses & 11776 \\
        libxml2 & 10698 \\
        vim-minimal & 10689 \\
        openldap & 10503 \\
        krb5 & 8725 \\
        python2 & 8261 \\
        openjdk & 8231 \\ \bottomrule
    \end{tabular}
    }
    \hfill
    \subfloat[CVEs by package]{
    \begin{tabular}{lr} \toprule
        \textbf{Package} & \textbf{CVEs} \\ \midrule
        kernel & 1822 \\
        linux & 1588 \\
        mysql & 998 \\
        linux-tools-common & 832 \\
        openjdk & 797 \\
        firefox & 679 \\
        imagemagick & 573 \\
        chromium-browser & 563 \\
        tensorflow & 399 \\
        tensorflow-gpu & 341 \\
        binutils & 238 \\
        linux-azure & 224 \\
        qemu & 214 \\
        tensorflow-cpu & 209 \\
        tiff & 202 \\
        vim & 181 \\
        mariadb & 171 \\
        tcpdump & 164 \\
        libtiff & 142 \\
        openssl & 137 \\ \bottomrule
    \end{tabular}
    }
    \hfill
    \subfloat[Images containing packages]{
    \begin{tabular}{lr} \toprule
        \textbf{Package} & \textbf{Tags} \\ \midrule
        openssl & 2366 \\
        ncurses & 1735 \\
        curl & 1641 \\
        zlib & 1523 \\
        tar & 1482 \\
        glibc & 1429 \\
        systemd & 1402 \\
        krb5 & 1366 \\
        gnutls & 1337 \\
        sqlite & 1258 \\
        coreutils & 1253 \\
        expat & 1243 \\
        perl & 1200 \\
        nghttp2 & 1197 \\
        openldap & 1194 \\
        passwd & 1177 \\
        gpgv & 1170 \\
        pam & 1146 \\
        pcre3 & 1129 \\
        bash & 1112 \\ \bottomrule
    \end{tabular}
    }
    \caption{Occurrences of packages}
    \label{tab:packages}
}
\end{table}

Overall, only 7\% of the evaluated tags do not contain any vulnerabilities. Approximately 40\% of the vulnerable images contain at least one critical vulnerability, and another 50\% contain at least one vulnerability that has a high severity score according to the CVSS scoring system. This information is illustrated in Figure \ref{img:tagscvescore}. The most vulnerable images are listed in Table \ref{tab:imagescve}.

\image{tagscvescore.png}{Severity of Vulnerable Images}{img:tagscvescore}{0.7}

\begin{table}[htb]
\centering
\begin{tabular}{lcrr} \toprule
    \textbf{Package} & \textbf{Last update} & \textbf{Pulls} & \textbf{CVEs} \\ \midrule
        nexcess/php-fpm & 2020 & 41M & 3131 \\
        appuio/s2i-maven-java & 2018 & 30M & 2943 \\
        stevenctimm/gpgridvanilla & 2017 & 44M & 2688 \\
        civisanalytics/civis-jupyter-r & 2020 & 49M & 2664 \\
        eclipse/che-dev & 2019 & 92M & 2639 \\
        civisanalytics/civis-jupyter-python3 & 2020 & 50M & 2205 \\
        broadinstitute/gatk & 2020 & 33M & 2101 \\
        scrapinghub/splash & 2017 & 73M & 1824 \\
        civisanalytics/civis-jupyter-python2 & 2019 & 48M & 1818 \\
        bugswarm/artifacts & 2018 & 165M & 1809 \\
        broadinstitute/gatk & 2019 & 33M & 1794 \\
        elgalu/selenium & 2017 & 111M & 1785 \\
        civisanalytics/civis-services-shiny & 2020 & 49M & 1780 \\
        avinetworks/controller & 2020 & 32M & 1757 \\
        czero/cflinuxfs2 & 2017 & 36M & 1751 \\ \bottomrule
\end{tabular}
\caption{Top vulnerable images}
\label{tab:imagescve}
\end{table}

However, no significant correlation could be inferred from Figure \ref{img:imagescvessc} between the popularity of an image and the number of vulnerabilities it contains.

\image{imagescvessc.png}{Number of vulnerabilities and image popularity}{img:imagescvessc}{0.7}

\FloatBarrier
\clearpage

\subsubsection{Quality of scanning tools}

After analyzing the data obtained from the scanning tools and understanding it, we can move on to evaluating the quality performance of the scanners.

As described in the previous section, all records from the perspective of each scanner can be categorized into four groups based on the results from its peers. These categories are True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN). Figure \ref{img:tp2} shows a table with the records collected by each scanner for each image. The number of true positive records is shown in the corresponding cell. The same calculations are performed for the other three categories.

\image{tp2.png}{Scanner stats}{img:tp2}{0.65}

The next step is to perform a metric calculation based on the received number of records. The equations necessary for this calculation were discussed in the previous section. The results of the calculation of Precision, Recall and $F_1$-score are presented in Table \ref{tab:calc} (a).


\begin{table}[htb]
    \centering
    {\small
    \subfloat[Minimum confirmation number — 2]{
        \begin{tabular}{lccc} \toprule
            \textbf{Tool} & \textbf{Precision} & \textbf{Recall} & \textbf{$F_1$} \\ \midrule
            Clair & 0.91 & 0.58 & 0.61 \\
            Google & 0.9 & 0.88 & 0.85 \\
            Grype & 0.68 & 0.89 & 0.75 \\
            Scout & 0.89 & 0.75 & 0.75 \\
            Snyk & 0.95 & 0.69 & 0.73 \\
            Trivy & 0.92 & 0.76 & 0.78 \\ \bottomrule
        \end{tabular}
    }
    \hfill
    \subfloat[Minimum confirmation number — 1]{
        \begin{tabular}{lccc} \toprule
            \textbf{Tool} & \textbf{Precision} & \textbf{Recall} & \textbf{$F_1$} \\ \midrule
            Clair & 0.94 & 0.51 & 0.57 \\
            Google & 0.97 & 0.8 & 0.83 \\
            Grype & 0.78 & 0.87 & 0.79 \\
            Scout & 0.96 & 0.68 & 0.73 \\
            Snyk & 0.98 & 0.61 & 0.68 \\
            Trivy & 0.98 & 0.7 & 0.76 \\ \bottomrule
        \end{tabular}
    }
    \caption{Quality metrics}
    \label{tab:calc}
}
\end{table}


Let us consider the visualisation of the results in Figure \ref{img:precisionrecall2}. As can be seen, all the tools demonstrate outstanding precision values, with all of them falling between 0.89 and 0.95. This means that among the vulnerabilities reported by each scanner, they are mostly correct and, in fact, exist in the image. This is at least as can be judged from the aggregated results of all the tools. Snyk demonstrates the highest result in this test. Next, the recall metric is one that not all scanning tools demonstrate well. The worst result among the evaluated tools is demonstrated by Clair — 0.58. In other words, approximately 40\% of the vulnerabilities reported by Clair were not actually validated by other tools and are therefore considered incorrect. Other tools perform better, with results ranging from 0.7 to 0.9.


The overall performance of the scanners can be evaluated using the $F_1$ metric. This composite benchmark depends on the precision and recall values, and it provides a single number to demonstrate the performance. As shown in Figure \ref{img:f12}, all scanners are assessed in the range from 0.61 to 0.85, with Clair producing the lowest result again. This is due to the insufficient recall value of Clair, which influenced the overall metric. The best performance according to the $F_1$-score is demonstrated by Google Artifact Registry, as it presented good results in both precision and recall. Other tools also have comparable results around 0.75.

\image{precisionrecall2.png}{Precision and Recall (number of confirmations — 2)}{img:precisionrecall2}{0.65}
\image{f12.png}{$F_1$-score (number of confirmations — 2)}{img:f12}{0.59}

Let us compare the results when the minimum number of validations required from neighbouring scanners was reduced from 2 to 1. This means that the criteria for considering a record to be correct are more relaxed. The results for this case are presented in Table \ref{tab:calc} (b), Figures \ref{img:precisionrecall1} and \ref{img:f11}. From the table, we can see that the precision values have increased as a result of the reduced number of confirmations, which indicates improved sensitivity. However, with this increase in precision, we also observe a decrease in recall values. In this scenario, half of the vulnerabilities reported by Clair may be incorrect. Nevertheless, the $F_1$ score remains relatively similar to the previous case.



\image{precisionrecall1.png}{Precision and Recall (number of confirmations — 1)}{img:precisionrecall1}{0.7}
\image{f11.png}{$F_1$-score (number of confirmations — 1)}{img:f11}{0.7}

\subsection{Outcomes}

In the previous section, we discussed the analysis of a sample of approximately 1,200 popular images and 3,200 tags from Docker Hub. Based on the aggregated data from Clair, Trivy, Docker Scout, Anchore Grype, Snyk, and Google Artifact Registry, we found that 93\% of the evaluated images contained at least one vulnerability. Most images contained at least one high or critical vulnerability, according to the CVSS severity scoring system. Additionally, it was discovered that the number of vulnerabilities in images has been increasing each year. This demonstrates the importance of static vulnerability analysis for images running in production environments. Installing a reliable pipeline that ensures that any known vulnerabilities are not present in the images can be an effective way to ensure the safety of containerized applications.

The results of the quality assessment conducted on the six vulnerability scanners mentioned are also promising. It was found that they correctly identify approximately 80\% of the vulnerabilities reported to security databases. Among the tools tested, Google Artifact Registry performed the best reporting with quality score of 0.83. Nevertheless, other tools, including open-source ones, also demonstrated comparable performance.

The comparison between precision and recall metrics raises the question of whether it is preferable to never miss an actual vulnerability, but occasionally report false positives, or whether it is better to never report false negatives, but risk missing important and real breaches.

It is reasonable to consider that the answer to this question depends on the criticality of the services being deployed and the cost of support. For services that are exposed to the internet and play a critical role in an organisation, it may be better to stick on the side of caution and not miss any potential threats. In less critical cases, it may be acceptable to reduce the number of false positives and overall reported vulnerabilities.

Hopefully, the results of our analysis will be helpful in choosing the right tool.



% предложить мол что выбирать какую ошибку. если уязвимость критическая лучше её передетектить. если уязвимость не критическая то лучше недо детектить чтобы снизить нагрузку на проверку руками и патчи и работу людей

% Both precision and recall may be useful in cases where there is imbalanced data. However, it may be valuable to prioritize one over the other in cases where the outcome of a false positive or false negative is costly. For example, in medical diagnosis, a false positive test can lead to unnecessary treatment and expenses. In this situation, it is useful to value precision over recall. In other cases, the cost of a false negative is high. For instance, the cost of a false negative in fraud detection is high, as failing to detect a fraudulent transaction can result in significant financial loss

% mention my github ezzzi