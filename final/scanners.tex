\section{Vulnerability detection}

\subsection{Previous studies}

The researches upon the quality and effectiveness of vulnerabilities detection tools have already been attempted. One of the most compregensive works was published by Omar Javed and Salman Toor in 2021 \cite{arxiv:1}. The approach described in the paper was based on several open source scanners (namely, Clair, Anchore, and Microscanner). After inspecting 59 Docker images for Java-based applications, the authors calculate detection coverage and detection hit ratio metrics and conclude that the most accurate tool (Anchore) was omitting around 34\% of vulnerabilities. A major limitation of their work is a relatively small number of images used to evaluate the performance of scanning tools as well as the range of scanners examined.

Another paper by K. Brady et al. described the CI/CD pipeline which combined static and dynamic analysers \cite{c:2}. A set of 7 images was submitted to Clair and Anchore scanners and original dynamic scanner based on Docker-in-Docker approach. The results clearly show the importance of dynamic detection method, however, no direct comparison of Clair and Anchore was conducted. Similar research was conducted in 2021, however, a pair of SonarQube and VirusTotal was used \cite{c:3}.

Massive research was conducted in 2020 by security company Prevasio. As the report states, all 4 million of public images from Docker Hub were scanned with static analyser (Trivy) and dynamic analyser, and more than half of all images were discovered to have critical vulnerabilities \cite{report:dynamic}. Significant number of images were containing dynamically downloaded payload, cryptominers and other malicious software.

\subsection{Static analysers}

Vulnerability scanners are special tools that identify known software vulnerabilities inside containers. Two approaches to this issue are known. First, it is possible to analyse versions of all the software in container image and check its safety. This approach is called static analysis, as it does not require any container to run, which speads up the process. However, static analysis has significant limitations, because the vulnerability databases take time to include recently discovered vulnerabilities, and container must not download any other packages besides what is included in the image. Finally, malicious but not yet reported packages cannot be discovered by static analysers. Nevertheless, this technology is extensively used in CI/CD pipelines to minimize risks of attack and container compromisation.

The opposite way to detect vulnerabilities in containers is to observe its runtime behaviour. Tools implementing this idea are called dynamic analysers. They may gather data about running processes, network and disk usage and discover suspicious activity patterns such as requests to C2 servers or compiling executables. This approach can effectively detect malicious images in the cases when static scanners are powerless. However, they require extensive resources and time usage, as each scan takes tens of seconds to complete, and may be unpredictable in special cases, when malicious applications disguise themselves and imitate normal behaviour unless started in the production environment. Further discussion will be held about static analysers.

The process of image attestation with static analyser may be described as following. First, the scanner compiles the list of all installed packages and libraries inside the image along with their versions. Next, each package is checked upon publically available vulnreability databases and marked as vulnerable or safe to use. This explains why various mistakes could easily occur during the detection process, or why reports from different scanners are not identical. Each scanner gets vulnerability information from various sources including per-distribution security advisories (Ubuntu Oval database, Debian Security Tracker, RHEL Oval database, to name but a few) or general vulnerability databases such as NVD (National Vulnerability Database). The information in these sources are of various degrees of relevance. Special treatment must be provided to the vulnerabilities that will not be fixed due to its negligibility. Finally, names of the packages may vary between distributions. All these reasons lead to false negative results, when a vulnerable package is marked as save. On the contrary, some packages are organised in groups, and incorrect treatment of the whole group because of one package can probably cause false positive results. Overall, the scanning report shoud be carefully considered in each individual case \cite{book:rice}.

\subsection{Existing tools}

The range of static vulnerability scanning tools available on the market is wide. From this extensive selection, we have chosen several for further evaluation. For the experiment, it was crucial that the scanning tool could run locally via the command line, as this allows for automation using Python scripts and the ability to process thousands of images for testing. Additionally, we wanted to ensure that the tool was either free or offered a sufficient trial period to meet our needs. After careful consideration, we selected the following tools: Clair, Trivy, Docker Scout, Anchore Grype, Snyk, and one provided by a cloud provider known as Google Artifact Registry.

\subsubsection{Clair}

Clair is a static analysis tool developed by CoreOS in 2015 to uncover vulnerabilities in containers that had previously remained unnoticed \cite{s:clair2015}. After CoreOS was discontinued in 2020, Clair can be used either as a standalone solution or as part of Project Quay, a container image registry that offers additional features such as image scanning \cite{s:what-is-clair}.

Clair performs a static analysis of container images. It scans each layer of the container image and identifies the packages contained within it. These packages are then evaluated against public vulnerability databases to identify any known potential security breaches. 

To identify vulnerabilities, Clair uses the following security databases \cite{s:clair-intro2}:

\begin{itemize}
    \item Ubuntu Oval database
    \item Debian Security Tracker
    \item Red Hat Enterprise Linux (RHEL) Oval database
    \item SUSE Oval database
    \item Oracle Oval database
    \item Alpine SecDB database
    \item VMware Photon OS database
    \item Amazon Web Services (AWS) UpdateInfo
    \item Open Source Vulnerability (OSV) Database
\end{itemize}

However, in local installations, Clair only recieves information from Ubuntu, Debian, Red Hat Enterprise Linux (RHEL), Alpine and the Open Source Vulnerabilities (OSV) databases.

Structurally, the Clair backend, which is composed of several daemons, such as Indexer, Matcher, and Notifier, runs in the background \cite{d:clairdeployment}. Upon startup, it downloads and maintains a local list of vulnerabilities that is stored in a Postgres database. To interact with the Clair backend, a special client tool called clairctl is used. clairctl pulls an image archive, pushes image layers to the Clair API, receives the results and presents them in a selected format. The results can be displayed on the screen or saved as a JSON file in a machine-readable format for later processing.

The latter feature simplifies the integration of Clair into the CI/CD pipeline for building and shipping containers. However, due to its design, Clair requires a significant amount of time to fully compile its vulnerability database, ranging from 20 to 30 minutes. To speed up this process, enthusiasts attempted to distribute preconfigured database images. A corresponding project can be found on GitHub (\url{https://github.com/arminc/clair-local-scan}). However, the feasibility of this approach is questionable due to the lack of support.

To run Clair locally, we used the Docker images from the official repository (\url{https://github.com/quay/clair}). A special docker-compose file, listed in Appendix \ref{appendix:clair-compose}, helps to run the database and scanner containers. The \texttt{clairctl} command-line tool, which is included with the Clair package, pulls the image from Docker Hub and passes it to the Clair backend. The deployment process is described in the Clair documentation (\url{https://quay.github.io/clair/howto/getting_started.html}). An example of a command that needs to be executed to scan an image is provided below:

\begin{listing}[htp]
    \centering
    \begin{minipage}{0.95\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
clairctl report --out json fluent/fluent-bit:2.0.8-debug > report.json
        \end{minted}
    \end{minipage}
    \caption{Run Clair scanner}
    \label{lst:clair}
\end{listing}

Upon completing the analysis, Clair generates a report that includes several sections, including information about the examined packages, the detected Linux distribution in the base image, and a list of vulnerabilities. For each vulnerability, Clair provides information about its identification in a database (for example, the CVE number), severity and the packages where the vulnerability was found. Additionally, the report indicates whether a fix for the vulnerability has already been released.

Overall, Clair is one of most established solutions for static container analysis and can be effectively used to gain insight into what is built and deployed in production systems. While its modular architecture may make the installation process more complicated, it provides a wide range of customization options for different environments and scalability for large-scale deployment systems. Later, we will explore the results of using Clair and compare them to other scanning tools.

\subsubsection{Trivy}

Trivy is an open-source scanner developed by Aqua Security, a cloud security company founded in 2015. It is a replacement for Microscanner, another vulnerability detector developed previously by the same company. The main advantage of Trivy over other scanning tools is the simplicity of installation. To begin scanning, developers simply need to download an executable file from the vendor's website or the package manager. Trivy can scan not only container images, but also file systems, Git repositories, Virtual Machine images and Infrastructure as code files \cite{d:trivystart}. In addition to detecting vulnerabilities, Trivy also checks for misconfigurations, such as secrets or sensitive information stored in plain text.

To demonstrate the scanning process, the Trivy tool can be executed by using the command provided in Listing \ref{lst:trivy}.

\begin{listing}[htp]
    \centering
    \begin{minipage}{0.75\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
trivy image --format sarif -o report.json golang:1.12-alpine
        \end{minted}
    \end{minipage}
    \caption{Run Trivy scanner}
    \label{lst:trivy}
\end{listing}

Internally, Trivy collects information about vulnerabilities from a wide range of sources, including security databases from major Linux distributions. It supports approximately 15 base images and, in addition, language-specific components such as libraries for approximately 12 programming languages, including Python, Go and C++ \cite{d:trivyvulnerability}. It is especially impressive considering that all these processes take place automatically and do not require any user intervention.

Trivy can also be used as part of a CI/CD pipeline. It is compatible with major continuous integration systems, such as GitHub Actions, GitLab CI and Travis CI. To facilitate integration with other tools and environments, Trivy provides various output formats, including human-readable text, JSON, and SERIF, which is a standard format for static code analysis. The JSON report contains information about the tool that was used for scanning, the vulnerabilities found their location in the image and the severity level.

In conclusion, Trivy is a recently developed and actively maintained open-source tool that focuses on the user-friendly approach. It provides wide support for various distributions, programming languages and platforms. With its extensive range of scanning features, Trivy makes it an ideal choice for setting up static analysis on containerized applications.


\subsubsection{Docker Scout}

Docker Scout is a new container security solution from Docker. It replaces the legacy Docker Scan tool and was made available to the general public in October 2023 \cite{s:scoutga}. 

Docker Scout can be used as a standalone plugin. To run Docker Scout plugin locally, it must be installed and then invoked as shown in Listing \ref{lst:scout}. Alternatively, Docker Scout can be integrated with Docker Hub, where developers can view the results of scanning their images from the cloud. An example of the Docker Scout dashboard can be seen in Figure \ref{}.


\begin{listing}[htp]
    \centering
    \begin{minipage}{1\linewidth}
        \begin{minted}[linenos=false, tabsize=4]{bash}
# installation:
curl -fsSL \
    https://raw.githubusercontent.com/docker/scout-cli/main/install.sh \
    -o install-scout.sh
sh install-scout.sh

# usage:
docker scout cves --format sarif --output report.json \
    stakater/reloader:SNAPSHOT-PR-586-UBI-0db6f802
        \end{minted}
    \end{minipage}
    \caption{Run Docker Scout scanner}
    \label{lst:scout}
\end{listing}

Otherwise, Docker Scout supports integrations with several third-party systems, such as container registries (JFrog Artifactory, Amazon and Azure container regstries) and CI/CD platforms (GitHub Actions, GitLab and Jenkins).

As a data source, Docker Scout aggregates vulnerability information from various sources, including Linux distribution advisories, language-specific databases, GitHub and GitLab advisory systems as well as the National Vulnerability Database \cite{d:scoutdb}. Unlike other scanner tools, Docker Scout utilizes Package URLs (PURLs) rather than the Common Product Enumeration (CPE) system, which, according to its developers, significantly reduces the likelihood of false positive results.

For the purposes of our evaluation, we primarily focus on the \texttt{docker scout cves} command, which generates a vulnerability report for a selected image. This command is not limited to images, but can also be used for OCI bundles and local files. The tool's output can be generated in plain text, SARIF or SBOM (Software Bill of Materials) formats. The SERIF-formatted report is compatible with other tools.


\subsubsection{Anchore Grype}


\subsubsection{Snyk}




\subsubsection{Google Artifact Registry}











\subsection{Methodology}

The process of data acquisition and the following analysis for this part of research could be divided into several stages Firstly, a set of container images and tags for scanning was composed. For this purpose Docker Hub API were employed. Then, each image was submitted to each scanning tool and vulnerability report was stored on disk in JSON format. After scanning, reports were combined and number of vulnerabilities for each image was calculated. Based on this calculations the metrics of detection quality were aggregated for each scanning tool.

\subsubsection{Images selection}

To select the most popular (according to the number of pulls) images from Docker Hub, the following algorithm was developed.

At first, we composed a list of search queries. The list consists of combinations of two latin letters starting from \texttt{aa} and finishing with \texttt{zz}. Each of $26 \cdot 26 = 676$ combinations was then used to query the list of corresponding images from Docker Hub, as shown in Listing \ref{lst:images}. The result was then saved to the JSON file.
\begin{listing}[htp]
    \centering
    \begin{minipage}{0.8\linewidth}
        \begin{minted}[linenos=true, tabsize=4]{python}
def get_page(page, query):
url = "https://hub.docker.com/api/content/v1/products/search"
params = {
    "page_size": 100,
    "q": query,
    "source": "community",
    "type": "image",
    "page": page
}
response = requests.get(url, params=params)
response.raise_for_status()
data = response.json()
return data["summaries"]
        \end{minted}
    \end{minipage}
    \caption{Query images}
    \label{lst:images}
\end{listing}

The next step is to combine search results from each query and sample a reasonably sized subset of the most popular images. We parse each JSON file and drop duplicating images. After all, the set of 3694651 images is sorted by \texttt{popularity} parameter and 0.999 percentile is selected. The resulting 3695 images are saved for further processing.

\subsubsection{Tags selection}

Each image exists in multiple versions which are referred to as image tags. We have to select a number of tags for each image to scan. Tags could also be built for a specific architecture and OS platform. We are primarily interested in \texttt{amd64} and \texttt{Linux} tags. 

The request used for generating the list of tags for each image is demonstrated in Listing \ref{lst:tags}. We randomly select 10 tags for each image that satisfy mentioned requirements. As the result, the set of approximately 30000 tags is composed and stored in file.

\begin{listing}[htp]
    \centering
    \begin{minipage}{1\linewidth}
        \begin{minted}[linenos=true, tabsize=4]{python}
pref, suf = image.split("/")
link = f"https://hub.docker.com/v2/namespaces/{pref}/repositories/{suf}/tags"
res = requests.get(link)
res = res.json()
up = res["count"]
iteration = 0
total = 0
while total < 10 and iteration < 100:
    page_num = random.randint(1, up)
    response = requests.get(link, params={"page": page_num, "page_size": 1})
    tag = response.json()["results"][0]
    if "amd64" in [i["architecture"] for i in tag["images"]]:
        tags.append(tag)
        total += 1
    iteration += 1
        \end{minted}
    \end{minipage}
    \caption{Query tags}
    \label{lst:tags}
\end{listing}

\subsubsection{Scan images}

Next steps were performed on a virtual machine with 16 vCPU and 32 Gb RAM. As a rule, each scanning tool must be installed on the machine, and then each image tag is passed to the tool with command line client.

\subsubsection*{Clair}




\subsubsection*{Scout}





\subsubsection*{Trivy} 




\subsubsection{Analysis of obtained scan reports}

After submitting the set of images to each of scanning tool, the reports must undergo the further analysis to determine the effectiveness of the studied software. As the results are presented in JSON format, python libraries such as \texttt{json} and \texttt{pandas} provide us with enough features to calculate the desired quality metrics. 

\subsubsection*{Quality metrics}

https://habr.com/ru/articles/661119/

\url{https://en.wikipedia.org/wiki/Precision_and_recall}